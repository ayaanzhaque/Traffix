{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "This is the basic data processing notebook. We will load our data from Kaggle and then do basic cleaning. We will save this data and use it in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required imports\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# set configurations\n",
    "pd.set_option('display.max_columns', 100)\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [16, 10]\n",
    "plt.rcParams['font.size'] = 14\n",
    "pd.set_option('display.max_columns', 99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first load the training and testing sets and begin to parse them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./data/train.csv') # this dataset is too large to push to git, so the zips are uploaded\n",
    "test = pd.read_csv('./data/test.csv') # this dataset is too large to push to git, so the zips are uploaded\n",
    "\n",
    "train['Istrain'] = 1\n",
    "test['Istrain'] = 0\n",
    "completeDataset = pd.concat([train, test], sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do basic encoding of characters to numbers. This includes encoding directions, road titles, weather, and temperature. We have trainign data from 4 cities (Atlanta, Boston, Chicago, and Philidelphia) and we will split part of it into validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_splits = pd.DataFrame([\n",
    "    ['Atlanta', 33.791, 33.835],\n",
    "    ['Boston', 42.361, 42.383],\n",
    "    ['Chicago', 41.921, 41.974],\n",
    "    ['Philadelphia', 39.999, 40.046],\n",
    "], columns=['City', 'l1', 'l2'])\n",
    "\n",
    "directionCodes = {\n",
    "    'N': 0,\n",
    "    'NE': 1 / 4,\n",
    "    'E': 1 / 2,\n",
    "    'SE': 3 / 4,\n",
    "    'S': 1,\n",
    "    'SW': 5 / 4,\n",
    "    'W': 3 / 2,\n",
    "    'NW': 7 / 4\n",
    "}\n",
    "\n",
    "road_encoding = {\n",
    "    'Road': 1,\n",
    "    'Street': 2,\n",
    "    'Avenue': 2,\n",
    "    'Drive': 3,\n",
    "    'Broad': 3,\n",
    "    'Boulevard': 4\n",
    "}\n",
    "\n",
    "monthly_rainfall = {\n",
    "    'Atlanta1': 5.02, 'Atlanta5': 3.95, 'Atlanta6': 3.63, 'Atlanta7': 5.12,\n",
    "    'Atlanta8': 3.67, 'Atlanta9': 4.09, 'Atlanta10': 3.11, 'Atlanta11': 4.10,\n",
    "    'Atlanta12': 3.82, 'Boston1': 3.92, 'Boston5': 3.24, 'Boston6': 3.22,\n",
    "    'Boston7': 3.06, 'Boston8': 3.37, 'Boston9': 3.47, 'Boston10': 3.79,\n",
    "    'Boston11': 3.98, 'Boston12': 3.73, 'Chicago1': 1.75, 'Chicago5': 3.38,\n",
    "    'Chicago6': 3.63, 'Chicago7': 3.51, 'Chicago8': 4.62, 'Chicago9': 3.27,\n",
    "    'Chicago10': 2.71, 'Chicago11': 3.01, 'Chicago12': 2.43,\n",
    "    'Philadelphia1': 3.52, 'Philadelphia5': 3.88, 'Philadelphia6': 3.29,\n",
    "    'Philadelphia7': 4.39, 'Philadelphia8': 3.82, 'Philadelphia9': 3.88,\n",
    "    'Philadelphia10': 2.75, 'Philadelphia11': 3.16, 'Philadelphia12': 3.31\n",
    "}\n",
    "\n",
    "monthly_temperature = {\n",
    "    'Atlanta1': 43, 'Atlanta5': 69, 'Atlanta6': 76, 'Atlanta7': 79,\n",
    "    'Atlanta8': 78, 'Atlanta9': 73, 'Atlanta10': 62, 'Atlanta11': 53,\n",
    "    'Atlanta12': 45, 'Boston1': 30, 'Boston5': 59, 'Boston6': 68, 'Boston7': 74,\n",
    "    'Boston8': 73, 'Boston9': 66, 'Boston10': 55, 'Boston11': 45,\n",
    "    'Boston12': 35, 'Chicago1': 27, 'Chicago5': 60, 'Chicago6': 70,\n",
    "    'Chicago7': 76, 'Chicago8': 76, 'Chicago9': 68, 'Chicago10': 56,\n",
    "    'Chicago11': 45, 'Chicago12': 32, 'Philadelphia1': 35, 'Philadelphia5': 66,\n",
    "    'Philadelphia6': 76, 'Philadelphia7': 81, 'Philadelphia8': 79,\n",
    "    'Philadelphia9': 72, 'Philadelphia10': 60, 'Philadelphia11': 49,\n",
    "    'Philadelphia12': 40}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will combine both parts of the dataset and use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Groups\n",
    "completeDataset = completeDataset.merge(validation_splits, on='City')\n",
    "completeDataset['ValidationGroup'] = 1\n",
    "completeDataset.loc[completeDataset.Latitude <= completeDataset.l1, 'ValidationGroup'] = 0\n",
    "completeDataset.loc[completeDataset.Latitude > completeDataset.l2, 'ValidationGroup'] = 2\n",
    "completeDataset.drop(['l1', 'l2'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the beginning of basic processing. We will collect any NaN (empty) values in the data and rounding values. We will also replace values with their encoded values. We also fill in any NaN with a string value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDataset['Latitude3'] = completeDataset.Latitude.round(3)\n",
    "completeDataset['Longitude3'] = completeDataset.Longitude.round(3)\n",
    "completeDataset['EntryStreetMissing'] = 1 * completeDataset.EntryStreetName.isna()\n",
    "completeDataset['ExitStreetMissing'] = 1 * completeDataset.ExitStreetName.isna()\n",
    "\n",
    "completeDataset['CMWH'] = completeDataset.City + '_' \\\n",
    "               + completeDataset.Month.astype(str) + '_' \\\n",
    "               + completeDataset.Weekend.astype(str) + '_' \\\n",
    "               + completeDataset.Hour.astype(str)\n",
    "\n",
    "completeDataset.EntryHeading = completeDataset.EntryHeading.replace(directionCodes)\n",
    "completeDataset.ExitHeading = completeDataset.ExitHeading.replace(directionCodes)\n",
    "completeDataset['DiffHeading'] = completeDataset['EntryHeading'] - completeDataset['ExitHeading']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDataset['city_month'] = completeDataset[\"City\"] + completeDataset[\"Month\"].astype(str)\n",
    "completeDataset[\"Rainfall\"] = completeDataset['city_month'].replace(monthly_rainfall)\n",
    "completeDataset[\"Temperature\"] = completeDataset['city_month'].replace(monthly_temperature)\n",
    "completeDataset.drop('city_month', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def road_encode(x):\n",
    "    for road in road_encoding.keys():\n",
    "        if road in x:\n",
    "            return road_encoding[road]\n",
    "    return 0\n",
    "\n",
    "completeDataset = completeDataset.fillna(dict(EntryStreetName='Unknown Something',\n",
    "                        ExitStreetName='Unknown Something'))\n",
    "\n",
    "completeDataset['EntryType'] = completeDataset['EntryStreetName'].apply(road_encode)\n",
    "completeDataset['ExitType'] = completeDataset['ExitStreetName'].apply(road_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDataset = completeDataset.fillna(dict(EntryStreetName='Unknown Something',\n",
    "                        ExitStreetName='Unknown Something'))\n",
    "\n",
    "completeDataset['EntryType'] = completeDataset['EntryStreetName'].apply(road_encode)\n",
    "completeDataset['ExitType'] = completeDataset['ExitStreetName'].apply(road_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDataset.EntryStreetName = completeDataset.City + ' ' + completeDataset.EntryStreetName\n",
    "completeDataset.ExitStreetName = completeDataset.City + ' ' + completeDataset.ExitStreetName\n",
    "completeDataset['Intersection'] = completeDataset.City + ' ' + completeDataset.IntersectionId.astype(str)\n",
    "\n",
    "completeDataset['SameStreet'] = 1 * (completeDataset.EntryStreetName == completeDataset.ExitStreetName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a geocoding algorithm. We will find the location of each intersection/datapoint and find its distance from the city center. This are just basic distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geolocation\n",
    "for col in ['Latitude', 'Longitude']:\n",
    "    scaler = StandardScaler()\n",
    "    completeDataset[col] = scaler.fit_transform(completeDataset[col].values.reshape(-1, 1))\n",
    "\n",
    "# Distance from CityCenter\n",
    "completeDataset = completeDataset.merge(\n",
    "    completeDataset.groupby('City')[['Latitude', 'Longitude']].mean(),\n",
    "    left_on='City', right_index=True, suffixes=['', 'Dist']\n",
    ")\n",
    "completeDataset.LatitudeDist = (5 * np.abs(completeDataset.Latitude - completeDataset.LatitudeDist)).round(3)\n",
    "completeDataset.LongitudeDist = (5 * np.abs(completeDataset.Longitude - completeDataset.LongitudeDist)).round(3)\n",
    "completeDataset['CenterDistL1'] = (5 * (completeDataset.LatitudeDist + completeDataset.LongitudeDist)).round(3)\n",
    "completeDataset['CenterDistL2'] = (3 * np.sqrt(\n",
    "    (completeDataset.LatitudeDist ** 2 + completeDataset.LongitudeDist ** 2))).round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_frequency(df, column):\n",
    "    cnt = df.groupby(column)[['RowId']].count()\n",
    "    cnt.loc[cnt.RowId > 10, 'RowId'] = 10 * (\n",
    "            cnt.loc[cnt.RowId > 10, 'RowId'] // 10)\n",
    "    cnt.columns = [f'{column}Count']\n",
    "    return df.merge(cnt, left_on=column, right_index=True)\n",
    "\n",
    "column_structures = ['Longitude3', 'Latitude3', 'ExitStreetName', 'EntryStreetName', 'Intersection', 'Path']\n",
    "\n",
    "for x in range(0,5): \n",
    "    completeDataset = add_frequency(completeDataset, column_structures[x])\n",
    "\n",
    "# Frequency Encoding with unique intersections\n",
    "def add_unique_intersections(df, column):\n",
    "    cnt = df.groupby(column)[['Intersection']].nunique()\n",
    "    cnt.loc[cnt.Intersection > 10, 'Intersection'] = 5 * (\n",
    "            cnt.loc[cnt.Intersection > 10, 'Intersection'] // 5)\n",
    "    cnt.columns = [f'{column}UniqueIntersections']\n",
    "    return df.merge(cnt, left_on=column, right_index=True)\n",
    "\n",
    "geo_column_structures = ['Longitude3', 'Latitude3', 'ExitStreetName', 'EntryStreetName']\n",
    "for x in range(0,3): \n",
    "    completeDataset = add_unique_intersections(completeDataset, geo_column_structures[x])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['City','EntryStreetName','ExitStreetName','Intersection', 'CMWH']\n",
    "\n",
    "for c in columns_to_encode:\n",
    "    encoder = LabelEncoder()\n",
    "    completeDataset[c] = encoder.fit_transform(completeDataset[c])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are done, we can export the features of this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "completeDataset.to_csv('./data/features_v3.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final check of our changes and processing to make sure everything is fine. Looks good here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainDataset = completeDataset[completeDataset.Istrain == 1].copy()\n",
    "test = completeDataset[completeDataset.Istrain == 0].copy()\n",
    "\n",
    "column_stats = pd.concat([\n",
    "    pd.DataFrame(completeDataset.count()).rename(columns={0: 'cnt'}),\n",
    "    pd.DataFrame(trainDataset.count()).rename(columns={0: 'trainDataset_cnt'}),\n",
    "    pd.DataFrame(test.count()).rename(columns={0: 'test_cnt'}),\n",
    "    pd.DataFrame(completeDataset.nunique()).rename(columns={0: 'unique'}),\n",
    "    pd.DataFrame(trainDataset.nunique()).rename(columns={0: 'trainDataset_unique'}),\n",
    "    pd.DataFrame(test.nunique()).rename(columns={0: 'test_unique'}),\n",
    "], sort=True, axis=1)\n",
    "\n",
    "\n",
    "column_stats['seen_in_trainDataset%'] = (\n",
    "            100 * column_stats.trainDataset_unique / column_stats.unique).round(1)\n",
    "column_stats = column_stats.sort_values(by='unique')\n",
    "\n",
    "\n",
    "column_stats.to_csv('data/col_stats.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
