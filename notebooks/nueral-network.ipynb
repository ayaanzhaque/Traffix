{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nueral Network\n",
    "This is another model we are testing for our application. A nueral network may perform better than an XG Boost model, so we will load the same data and see if we can get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# required imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tf imports\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow_addons.optimizers import RectifiedAdam, Lookahead\n",
    "from sklearn.base import BaseEstimator\n",
    "from tensorflow_addons.activations import gelu\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler, OrdinalEncoder, FunctionTransformer, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "Now we will build a customized and adaptable nueral network with keras. Since the input size can vary based on the attribute we are training on, we need to be able to change it based on the input shape. We preset the chosen hyperparemteres such as loss functions and activation functions. \n",
    "\n",
    "The first function \"create_model\" will allow us to build a model with the proper number of layers we want as well as the correct input sizes. This is called by the \"KerasModel\" function whic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(grid_params, in_dim, out_dim, patience=20, loss='rmse', activation='sigmoid'):\n",
    "    \n",
    "    mul_input = grid_params['mul_input']\n",
    "    n_layer = grid_params['n_layer']\n",
    "    \n",
    "    first_layer_size = int(in_dim*mul_input)\n",
    "    hidden_layers = []\n",
    "    for i_layer in range(n_layer, 0, -1):\n",
    "        layer_size = int(((first_layer_size - out_dim) / n_layer) * i_layer + out_dim)\n",
    "        hidden_layers.append(layer_size)\n",
    "\n",
    "    print(\"Input dim:\" + str(in_dim))\n",
    "    print(\"Hidden Layers:\" + str(hidden_layers))\n",
    "    print(\"Output dim:\" + str(out_dim))\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(in_dim,input_shape=[in_dim],activation=gelu))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    for layer in hidden_layers:\n",
    "        model.add(Dense(layer,activation=gelu))\n",
    "        #model.add(BatchNormalization())\n",
    "        model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Dense(out_dim, activation=activation))\n",
    "    \n",
    "    radam = RectifiedAdam()\n",
    "    ranger = Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "    optimizer = ranger#Adam(learning_rate=0.001)\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', verbose=1, mode='min', patience=patience, restore_best_weights=True)\n",
    "    es.set_model(model)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=[loss], metrics=[])\n",
    "    \n",
    "    return model, [ es ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KerasModel(BaseEstimator):\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_layer=1, \n",
    "        mul_input=1.75, \n",
    "        patience=5,\n",
    "        batch_size=32,\n",
    "        loss='msle',\n",
    "        activation='sigmoid'\n",
    "        ):\n",
    "        self._estimator_type = 'reg' \n",
    "        self.n_layer = n_layer\n",
    "        self.mul_input = mul_input\n",
    "        self.patience = patience\n",
    "        self.loss = loss\n",
    "        self.activation = activation\n",
    "        self.batch_size = batch_size\n",
    "        #self.__name__ = self._wrapped_obj.__class__.__name__ + \"PredictWrapper\"\n",
    "\n",
    "    def __repr__(self):\n",
    "        if not hasattr(self, 'model'):\n",
    "            return \"Empty\"\n",
    "        return self.model.__repr__()\n",
    "\n",
    "    def __str__(self):\n",
    "        if not hasattr(self, 'model'):\n",
    "            return \"Empty\"\n",
    "        return self.model.__str__()\n",
    "        \n",
    "    def fit(self, X, Y, x_val, y_val):\n",
    "        model, cbs = create_model(\n",
    "            self.get_params(),\n",
    "            X.shape[1],\n",
    "            Y.shape[0],\n",
    "            patience=self.patience,\n",
    "            loss=self.loss,\n",
    "            activation=self.activation\n",
    "        )\n",
    "#         X_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2, random_state=42, shuffle=True)\n",
    "        self.model = model\n",
    "        self.model.fit(X_train,y_train, batch_size=self.batch_size,epochs=10000, validation_data=[X_val,y_val], verbose=2, callbacks=cbs)\n",
    "        return self\n",
    "\n",
    "    def predict(self, *args, **kwargs):\n",
    "        return self.model.predict(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DistanceToFirstStop_p20', 'DistanceToFirstStop_p40', 'DistanceToFirstStop_p50', 'DistanceToFirstStop_p60', 'DistanceToFirstStop_p80', 'TimeFromFirstStop_p20', 'TimeFromFirstStop_p40', 'TimeFromFirstStop_p50', 'TimeFromFirstStop_p60', 'TimeFromFirstStop_p80', 'TotalTimeStopped_p20', 'TotalTimeStopped_p40', 'TotalTimeStopped_p50', 'TotalTimeStopped_p60', 'TotalTimeStopped_p80']\n",
      "35\n",
      "['City', 'EntryHeading', 'EntryStreetName', 'ExitHeading', 'ExitStreetName', 'Hour', 'Istrain', 'Latitude', 'Longitude', 'Month', 'Weekend', 'Latitude3', 'Longitude3', 'EntryStreetMissing', 'ExitStreetMissing', 'CMWH', 'DiffHeading', 'Rainfall', 'Temperature', 'EntryType', 'ExitType', 'Intersection', 'SameStreet', 'LatitudeDist', 'LongitudeDist', 'CenterDistL1', 'CenterDistL2', 'Longitude3Count', 'Latitude3Count', 'ExitStreetNameCount', 'EntryStreetNameCount', 'IntersectionCount', 'Longitude3UniqueIntersections', 'Latitude3UniqueIntersections', 'ExitStreetNameUniqueIntersections']\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "# This is the same loading code as before, so refer to the other notebooks for reference.\n",
    "full = pd.read_csv('./data/features_v3.csv.gz')\n",
    "full.head()\n",
    "\n",
    "full['random'] = np.random.rand(len(full))\n",
    "\n",
    "TRAIN_SAMPLE_SIZE = 0.75\n",
    "\n",
    "train = full[full.Istrain == 1]\n",
    "test = full[full.Istrain == 0]\n",
    "\n",
    "column_stats = pd.concat([\n",
    "    pd.DataFrame(full.count()).rename(columns={0: 'cnt'}),\n",
    "    pd.DataFrame(full.nunique()).rename(columns={0: 'unique'}),\n",
    "], sort=True, axis=1)\n",
    "column_stats.sort_values(by='unique')\n",
    "\n",
    "train_columns = list(column_stats[column_stats.cnt < 10 ** 6].index)\n",
    "print(train_columns)\n",
    "\n",
    "target_columns = [\n",
    "    'TotalTimeStopped_p20',\n",
    "    'TotalTimeStopped_p50',\n",
    "    'TotalTimeStopped_p80',\n",
    "    'DistanceToFirstStop_p20',\n",
    "    'DistanceToFirstStop_p50',\n",
    "    'DistanceToFirstStop_p80',\n",
    "]\n",
    "\n",
    "do_not_use = train_columns + ['IsTrain', 'Path', 'RowId', 'IntersectionId',\n",
    "                              'random', 'intersection_random', 'ValidationGroup']\n",
    "\n",
    "feature_columns = [c for c in full.columns if c not in do_not_use]\n",
    "print(len(feature_columns))\n",
    "print(feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "First we will initialize the model with the chosen hyperparameters. Then we will train it on the specific attributes and save the best model, and finally we will run validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KerasModel(n_layer=3, mul_input=8, batch_size=1024, patience=10, activation=None, loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will loop through the target columns and train the model iteratively on each. We will also save the best model each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and predicting for target TotalTimeStopped_p20\n",
      "(642086, 35) (642086,) (214301, 35) (214301,)\n",
      "Input dim:35\n",
      "Hidden Layers:[280, 214215, 428150]\n",
      "Output dim:642086\n"
     ]
    }
   ],
   "source": [
    "for i, target in enumerate(target_columns):\n",
    "    print(f'Training and predicting for target {target}')\n",
    "    train_idx = train.random < TRAIN_SAMPLE_SIZE\n",
    "    valid_idx = train.random >= TRAIN_SAMPLE_SIZE\n",
    "\n",
    "    Xtr = train[train_idx][feature_columns]\n",
    "    Xv = train[valid_idx][feature_columns]\n",
    "    ytr = train[train_idx][target].values\n",
    "    yv = train[valid_idx][target].values\n",
    "    print(Xtr.shape, ytr.shape, Xv.shape, yv.shape)\n",
    "\n",
    "    mc = ModelCheckpoint('.models/nn.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "    \n",
    "    history = model.fit(Xtr, ytr, Xv, yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation code\n",
    "Y_test = pipeline.predict(X_test)\n",
    "res_df = pd.DataFrame(data=Y_test, columns=targets)\n",
    "res_df['RowId'] = X_test['RowId']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
